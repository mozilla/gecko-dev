From 0f02f0a222923dde8f7d81a8f2667dc6c15e8330 Mon Sep 17 00:00:00 2001
From: serge-sans-paille <sguelton@mozilla.com>
Date: Tue, 13 May 2025 12:13:22 +0200
Subject: [PATCH 2/5] Replace gsl::narrow by static_cast, as the former can
 throw exception

---
 .../core/framework/kernel_type_str_resolver.cc       |  2 +-
 .../graph/runtime_optimization_record_container.cc   |  2 +-
 onnxruntime/core/optimizer/attention_fusion.cc       |  8 ++++----
 .../core/optimizer/embed_layer_norm_fusion.cc        |  6 +++---
 onnxruntime/core/optimizer/nchwc_transformer.cc      | 12 ++++++------
 onnxruntime/core/optimizer/utils.cc                  |  4 ++--
 6 files changed, 17 insertions(+), 17 deletions(-)

diff --git a/onnxruntime/core/framework/kernel_type_str_resolver.cc b/onnxruntime/core/framework/kernel_type_str_resolver.cc
index 3142f94f28..b64ed160da 100644
--- a/onnxruntime/core/framework/kernel_type_str_resolver.cc
+++ b/onnxruntime/core/framework/kernel_type_str_resolver.cc
@@ -167,7 +167,7 @@ Status KernelTypeStrResolver::SaveToOrtFormat(
         auto fbs_arg = fbs::CreateArgTypeAndIndex(
             builder,
             arg.first == ArgType::kInput ? fbs::ArgType::INPUT : fbs::ArgType::OUTPUT,
-            gsl::narrow<uint32_t>(arg.second));
+            static_cast<uint32_t>(arg.second));
         fbs_args.push_back(fbs_arg);
       }
 
diff --git a/onnxruntime/core/graph/runtime_optimization_record_container.cc b/onnxruntime/core/graph/runtime_optimization_record_container.cc
index 2d0e1076ee..36a0f37a27 100644
--- a/onnxruntime/core/graph/runtime_optimization_record_container.cc
+++ b/onnxruntime/core/graph/runtime_optimization_record_container.cc
@@ -57,7 +57,7 @@ static Status SaveRuntimeOptimizationRecordToOrtFormat(
 
   const auto fbs_node_indices = builder.CreateVector<uint32_t>(
       nodes_to_optimize_indices.nodes.size(),
-      [&](size_t i) { return gsl::narrow<uint32_t>(nodes_to_optimize_indices.nodes[i]); });
+      [&](size_t i) { return static_cast<uint32_t>(nodes_to_optimize_indices.nodes[i]); });
 
   const auto fbs_nodes_to_optimize =
       fbs::CreateNodesToOptimizeIndices(builder,
diff --git a/onnxruntime/core/optimizer/attention_fusion.cc b/onnxruntime/core/optimizer/attention_fusion.cc
index ff8943de79..6186fb542f 100644
--- a/onnxruntime/core/optimizer/attention_fusion.cc
+++ b/onnxruntime/core/optimizer/attention_fusion.cc
@@ -121,25 +121,25 @@ static NodeArg& MergeQkvWeights(Graph& graph, int64_t hidden_size,
     const float* k_weight = k_initializer.data<float>();
     const float* v_weight = v_initializer.data<float>();
     std::vector<float> result;
-    result.reserve(gsl::narrow<size_t>(element_count));
+    result.reserve(static_cast<size_t>(element_count));
     if (is_matmul) {
       MergeMatMulWeights<float>(q_weight, k_weight, v_weight, result, hidden_size);
     } else {
       MergeWeights<float>(q_weight, k_weight, v_weight, result, hidden_size);
     }
-    utils::SetRawDataInTensorProto(initializer, result.data(), gsl::narrow<size_t>(element_count) * sizeof(float));
+    utils::SetRawDataInTensorProto(initializer, result.data(), static_cast<size_t>(element_count) * sizeof(float));
   } else {  // data_type == ONNX_NAMESPACE::TensorProto_DataType_FLOAT16
     const MLFloat16* q_weight = q_initializer.data<MLFloat16>();
     const MLFloat16* k_weight = k_initializer.data<MLFloat16>();
     const MLFloat16* v_weight = v_initializer.data<MLFloat16>();
     std::vector<MLFloat16> result;
-    result.reserve(gsl::narrow<size_t>(element_count));
+    result.reserve(static_cast<size_t>(element_count));
     if (is_matmul) {
       MergeMatMulWeights<MLFloat16>(q_weight, k_weight, v_weight, result, hidden_size);
     } else {
       MergeWeights<MLFloat16>(q_weight, k_weight, v_weight, result, hidden_size);
     }
-    utils::SetRawDataInTensorProto(initializer, result.data(), gsl::narrow<size_t>(element_count) * sizeof(MLFloat16));
+    utils::SetRawDataInTensorProto(initializer, result.data(), static_cast<size_t>(element_count) * sizeof(MLFloat16));
   }
 
   return graph_utils::AddInitializer(graph, initializer);
diff --git a/onnxruntime/core/optimizer/embed_layer_norm_fusion.cc b/onnxruntime/core/optimizer/embed_layer_norm_fusion.cc
index 103e72072f..d91529273e 100644
--- a/onnxruntime/core/optimizer/embed_layer_norm_fusion.cc
+++ b/onnxruntime/core/optimizer/embed_layer_norm_fusion.cc
@@ -431,7 +431,7 @@ template <typename T>
 bool CheckEmbeddingData(const T* data, int64_t batch_size, int64_t element_count) {
   // check that all batches has same data.
   size_t data_length = SafeInt<size_t>(batch_size) * element_count;
-  for (size_t i = gsl::narrow<size_t>(element_count); i < data_length; i++) {
+  for (size_t i = static_cast<size_t>(element_count); i < data_length; i++) {
     if (data[i] != data[i % element_count]) {
       return false;
     }
@@ -465,13 +465,13 @@ static NodeArg* ExtractEmbedding(Graph& graph,
     if (!CheckEmbeddingData(data, batch_size, element_count)) {
       return nullptr;
     }
-    utils::SetRawDataInTensorProto(initializer, data, gsl::narrow<size_t>(element_count) * sizeof(float));
+    utils::SetRawDataInTensorProto(initializer, data, static_cast<size_t>(element_count) * sizeof(float));
   } else {  // data_type == ONNX_NAMESPACE::TensorProto_DataType_FLOAT16
     const MLFloat16* data = old_initializer.data<MLFloat16>();
     if (!CheckEmbeddingData(data, batch_size, element_count)) {
       return nullptr;
     }
-    utils::SetRawDataInTensorProto(initializer, data, gsl::narrow<size_t>(element_count) * sizeof(MLFloat16));
+    utils::SetRawDataInTensorProto(initializer, data, static_cast<size_t>(element_count) * sizeof(MLFloat16));
   }
 
   NodeArg& node_arg = graph_utils::AddInitializer(graph, initializer);
diff --git a/onnxruntime/core/optimizer/nchwc_transformer.cc b/onnxruntime/core/optimizer/nchwc_transformer.cc
index 46f306b92b..436f16661f 100644
--- a/onnxruntime/core/optimizer/nchwc_transformer.cc
+++ b/onnxruntime/core/optimizer/nchwc_transformer.cc
@@ -415,7 +415,7 @@ void NchwcTransformerImpl::TransformConv(Node& node) {
     for (size_t i = 2; i < 4; i++) {
       reordered_filter_size *= conv_W_dims[i];
     }
-    InlinedVector<float> reordered_filter(gsl::narrow<size_t>(reordered_filter_size));
+    InlinedVector<float> reordered_filter(static_cast<size_t>(reordered_filter_size));
 
     // Reorder the weights tensor statically.
     if (reorder_filter_OIHWBo) {
@@ -451,7 +451,7 @@ void NchwcTransformerImpl::TransformConv(Node& node) {
     } else {
       Initializer conv_B{*conv_B_tensor_proto, graph_.ModelPath()};
 
-      InlinedVector<float> aligned_bias(gsl::narrow<size_t>(nchwc_output_channels));
+      InlinedVector<float> aligned_bias(static_cast<size_t>(nchwc_output_channels));
       ORT_ENFORCE(output_channels <= nchwc_output_channels, "Buffer overflow");
       std::copy_n(conv_B.data<float>(), output_channels, aligned_bias.data());
 
@@ -460,7 +460,7 @@ void NchwcTransformerImpl::TransformConv(Node& node) {
       nchwc_conv_B_tensor_proto.set_data_type(ONNX_NAMESPACE::TensorProto_DataType_FLOAT);
       nchwc_conv_B_tensor_proto.set_name(graph_.GenerateNodeArgName("reorder"));
       utils::SetRawDataInTensorProto(nchwc_conv_B_tensor_proto, aligned_bias.data(),
-                                     gsl::narrow<size_t>(nchwc_output_channels) * sizeof(float));
+                                     static_cast<size_t>(nchwc_output_channels) * sizeof(float));
 
       nchwc_conv_B_tensor_proto.add_dims(nchwc_output_channels);
 
@@ -878,7 +878,7 @@ void NchwcTransformerImpl::TransformBatchNormalization(Node& node) {
   const size_t nchwc_block_size = MlasNchwcGetBlockSize();
   const int64_t nchwc_channels = (channels + nchwc_block_size - 1) & ~(nchwc_block_size - 1);
 
-  InlinedVector<float> padded_buffer(gsl::narrow<size_t>(nchwc_channels));
+  InlinedVector<float> padded_buffer(static_cast<size_t>(nchwc_channels));
 
   std::copy_n(bn_scale.data<float>(), channels, padded_buffer.data());
 
@@ -886,7 +886,7 @@ void NchwcTransformerImpl::TransformBatchNormalization(Node& node) {
   nchwc_conv_W_tensor_proto.set_data_type(ONNX_NAMESPACE::TensorProto_DataType_FLOAT);
   nchwc_conv_W_tensor_proto.set_name(graph_.GenerateNodeArgName("bn_scale"));
   utils::SetRawDataInTensorProto(nchwc_conv_W_tensor_proto, padded_buffer.data(),
-                                 gsl::narrow<size_t>(nchwc_channels) * sizeof(float));
+                                 static_cast<size_t>(nchwc_channels) * sizeof(float));
   nchwc_conv_W_tensor_proto.add_dims(nchwc_channels);
   nchwc_conv_W_tensor_proto.add_dims(1);
   nchwc_conv_W_tensor_proto.add_dims(1);
@@ -900,7 +900,7 @@ void NchwcTransformerImpl::TransformBatchNormalization(Node& node) {
   nchwc_conv_B_tensor_proto.set_data_type(ONNX_NAMESPACE::TensorProto_DataType_FLOAT);
   nchwc_conv_B_tensor_proto.set_name(graph_.GenerateNodeArgName("bn_B"));
   utils::SetRawDataInTensorProto(nchwc_conv_B_tensor_proto, padded_buffer.data(),
-                                 gsl::narrow<size_t>(nchwc_channels) * sizeof(float));
+                                 static_cast<size_t>(nchwc_channels) * sizeof(float));
   nchwc_conv_B_tensor_proto.add_dims(nchwc_channels);
 
   auto* nchwc_conv_B_arg = &graph_utils::AddInitializer(graph_, nchwc_conv_B_tensor_proto);
diff --git a/onnxruntime/core/optimizer/utils.cc b/onnxruntime/core/optimizer/utils.cc
index c7e11de348..047d095aaf 100644
--- a/onnxruntime/core/optimizer/utils.cc
+++ b/onnxruntime/core/optimizer/utils.cc
@@ -175,11 +175,11 @@ bool AppendTensorFromInitializer(const Graph& graph, const NodeArg& input_arg, I
   const auto data_type = tensor_proto->data_type();
   if (data_type == ONNX_NAMESPACE::TensorProto_DataType_INT64) {
     const int64_t* val = init_const.data<int64_t>();
-    data.reserve(data.size() + gsl::narrow<size_t>(init_const.size()));
+    data.reserve(data.size() + static_cast<size_t>(init_const.size()));
     data.insert(data.end(), val, val + init_const.size());
   } else if (data_type == ONNX_NAMESPACE::TensorProto_DataType_INT32) {
     const int32_t* val = init_const.data<int32_t>();
-    data.reserve(data.size() + gsl::narrow<size_t>(init_const.size()));
+    data.reserve(data.size() + static_cast<size_t>(init_const.size()));
     for (size_t i = 0; i < init_const.size(); i++) {
       data.push_back(static_cast<int64_t>(val[i]));
     }
-- 
2.49.0

